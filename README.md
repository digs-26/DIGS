# SASS #

This is the implementation repository of paper: 

Not All Synthetic Vulnerabilities Are Equal: Synthetic Vulnerability Selection for Better Learning Deep Vulnerability Detectors


## Description ##

We present SASS, a novel approach to improve deep
vulnerability detector training by selecting synthetic vulnerabilities through assessing their intra-dataset similarity and inter-dataset similrity scores.

## Reproducibility ##
### Requirements ###
- Python==3.7.13
- torch==1.12.1
- transformers==4.25.1
- tqdm==4.62.3
- numpy==1.21.5
- scikit-learn==1.0.2
- sinularity-ce==4.2.2 (for running the evaluation code)

### Structure ###
    |-SASS/ "implementation for SASS".
    |-evaluation/ "contains the code for evaluating the SASS and baseline approaches."
    |   |-devign/ 
    |   |-linevul/
    |   |-linevd/
    |   |-velvet/
    |-results/ "tables of experimental results."

### Usage ###
Selecting synthetic vulnerabilities with SASS:
1. Download the original datasets and synthetic datasets by following the links in **Dataset** part. 
2. Set the database path, output path, the select ratio in `/SASS/select.sh`.
3. Run `/SASS/select.sh` to generate the synthetic subset indicies selected by SASS.

Reproduce the evaluation results for vulnerability detectors:
1. We provide the data and model files mentioned in our experiments in the following links: https://drive.google.com/drive/u/3/folders/1NQPz1mcN9EC4bR-SBraz2A5Ncdi5J8Wp Please download them. 
2. Cd to the evaluation folder, set the synthetic subset indicies path and the dataset path in the scripts or the main file of the code, following the help of scricpts `run_linevul.sh`, `run_devign.sh`, `run_linevd.sh`, `run_velvet.sh`, training and testing detectors. 
3. We provide the compared baseline approaches code file under the `/SASS` folder. Change the method name in the `/SASS/select.sh` to run the baseline approaches.
4. Please note that Devign, LineVD and VELVET need a  long-time preprocess. We provide the preprocess scripts in each detector's folder.

### Dataset ###

#### Original vulnerabiltiy datasets ####

- Devign (a real-world vulnerable dataset collected from FFmpeg and QEMU) [1]
- ReVeal (a real-world vulnerable dataset collected from Debian and Chromium) [2]
- Big-Vul (a large scale real-world vulnerable dataset collected from CVE database) [3]

#### Synthetic vulnerability datasets ####

- VulGen (a large scale synthetic dataset generated by injecting vulnerability patterns to normal code) [4]
- VGX (an improved synthetic vulnerability dataset with manual validation and advanced localization model) [5]
- VulScriber (LLM based synthetic vulnerability dataset, combine the RAG and prompt engineering) [6]
- Juliet (a large manually crafted synthetic vulnerabilities designed to benchmark detection tools across)

[1] Devign https://sites.google.com/view/devign

[2] ReVeal https://drive.google.com/drive/folders/1KuIYgFcvWUXheDhT--cBALsfy1I4utOy

[3] Big-Vul https://drive.google.com/file/d/1-0VhnHBp9IGh90s2wCNjeCMuy70HPl8X/view

[4] VulGen https://zenodo.org/records/10574446

[5] VGX https://zenodo.org/records/7569854

[6] VulScriber https://github.com/VulScribeR/VulScribeR

[7] Juliet https://samate.nist.gov/SARD/test-suites/112